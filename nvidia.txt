# Set power limit
# ============================================================
# Check current range (check min, max, default)
nvidia-smi -q -d POWER

# Enable persistence and apply the cap
sudo nvidia-smi -pm 1
sudo nvidia-smi -i 0 -pl <WATTS>

# Set limit on reboot.
# The following example set to 500W
sudo tee /etc/systemd/system/nvidia-power-limit.service >/dev/null <<'EOF'
[Unit]
Description=Set NVIDIA GPU Power Limit
After=multi-user.target

[Service]
Type=oneshot
ExecStart=/usr/bin/nvidia-smi -pm 1
ExecStart=/usr/bin/nvidia-smi -i 0 -pl 500

[Install]
WantedBy=multi-user.target
EOF

# Check
sudo journalctl -f -u nvidia-power-limit # Run in another terminal to check log
sudo systemctl start nvidia-power-limit
sudo systemctl status nvidia-power-limit

# Run on reboot
sudo systemctl enable nvidia-power-limit.service

# ============================================================

# Patching Nvidia .run file
./NVIDIA-Linux-x86_64-375.39.run -x
cd NVIDIA-Linux-x86_64-375.39
patch -p1 < mypatchfile

# Enable persistence mode
# Resolve 100% CPU utilization issue when GPU is used with ECC memory
# https://stackoverflow.com/questions/52759509/100-gpu-utilization-on-a-gce-without-any-processes
sudo nvidia-smi -pm 1

# Test run Nvidia in Docker. Make sure the host OS has installed Nvidia drivers.
# https://github.com/NVIDIA/nvidia-docker
# ============================================================

# Run with CUDA 10 with cuDNN 7
docker run --rm -it --gpus all --name nvidia nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04 bash

# Once you have an interactive shell in the container.
# Install base utilities.
apt-get update && apt-get -y install curl git 

# Setup miniconda with Python 3.8 
curl -LO https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh -b -f
$HOME/miniconda3/bin/conda init
source ~/.bashrc

# Install Keras (has examples to easily test GPU is working) and Tensorflow
pip install keras==2.4.3 tensorflow==2.3.1

# Download and run examples from Keras. 
# Ensure there are no errors. Can run nvidia-smi separately to verify GPU usage.
git clone --single-branch --branch 2.4.0 https://github.com/keras-team/keras
cd keras
python examples/mnist_mlp.py

# Cuda not working after wake up from sleep
# https://forums.fast.ai/t/cuda-lib-not-working-after-suspend-on-ubuntu-16-04/3546/

cat <<EOF | sudo tee /etc/systemd/system/nvidia-reload.service

[Unit]
Description=reload Nvidia modules after wake up from sleep
After=syslog.target network.target suspend.target

[Service]
Type=oneshot
ExecStart=/bin/bash -c "rmmod nvidia_uvm && modprobe nvidia_uvm"

[Install]
WantedBy=suspend.target
EOF

sudo systemctl start nvidia-reload
sudo systemctl enable nvidia-reload

# Fan config in Linux
# https://wiki.archlinux.org/title/NVIDIA/Tips_and_tricks
# https://askubuntu.com/a/766111/1165335

# Enable option to control fan speed
sudo nvidia-xconfig --cool-bits=4

# Set fan curve and autostart on login, using user systemd
cat <<EOF > $HOME/.config/systemd/user/nvidia-fancurve.service
[Unit]
Description=Nvidia Fan Curve

[Service]
ExecStart=/home/dheryanto/etc/nvidia-fancurve-linux.py
Restart=on-failure
RestartSec=20s

[Install]
WantedBy=default.target
EOF

systemctl --user daemon-reload
systemctl --user enable --now nvidia-fancurve
systemctl --user status nvidia-fancurve
