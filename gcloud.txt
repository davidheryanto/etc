Basics
============================================================

Install gcloud (Debian based)
------------------------------------------------------------
apt-get -qq update \
&& apt-get -y install wget curl vim python \
&& wget -qO- https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-244.0.0-linux-x86_64.tar.gz | tar xz -C / \
&& export PATH=$PATH:/google-cloud-sdk/bin

# Install for local workstation
wget -qO- https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-201.0.0-linux-x86_64.tar.gz | tar xz -C ~/ && \
cat <<EOF >> ~/.bashrc

# Google Cloud
export PATH=$HOME/google-cloud-sdk/bin:\$PATH
EOF

Configuration
------------------------------------------------------------
gcloud config list
gcloud config configurations list

gcloud config configurations create CONFIGURATION_NAME [--no-activate]
gcloud config configurations activate [my_config]

# Setup different configurations
gcloud init
gcloud container clusters create k0 
gcloud config set compute/zone asia-east1-a

# Get account email 
gcloud info --format="value(config.account)"

Projects
------------------------------------------------------------
gcloud projects list
gcloud config set project <DEFAULT_PROJECT_ID>

Container Registry
------------------------------------------------------------
Delete untagged images
- https://stackoverflow.com/questions/46451173/delete-untagged-images-on-google-cloud-registry

gcloud container images list-tags gcr.io/project-id/repository --filter='-tags:*'  --format='get(digest)' --limit=$BIG_NUMBER|unlimited
# Need to loop all the digest, then
gcloud container images delete --quiet gcr.io/project-id/repository@DIGEST
gcloud --quiet container images delete gcr.io/project-id/repository@sha256:123456

Extra
============================================================
gcloud components install kubectl

gcloud containers cluster create 
--disk-size=DISK_SIZE 
--num-nodes
--tags
--zone

# Common environment variables to set
GOOGLE_APPLICATION_CREDENTIALS=/path/to/service_account.json
# Set default project
GCLOUD_PROJECT=my-gcloud-project

# Activate service account
gcloud auth activate-service-account --key-file=/path_to/key.json

# Not use the currentactivated service account anymore
gcloud auth revoke

# Kubernetes Engine setup credentials
gcloud container clusters get-credentials [CLUSTER_NAME]
gcloud container clusters get-credentials --internal-ip [--zone=xx] [CLUSTER_NAME]
gcloud container clusters get-credentials --zone=asia-east1-b --internal-ip [CLUSTER_NAME]

Using Google Application Default Credentials
=============================================================
# https://developers.google.com/identity/protocols/application-default-credentials
# Useful when you are developing code that would normally use a service account but need to run the code in a local development 
gcloud auth application-default login

# Revoke service account
gcloud auth revoke account@project.iam.gserviceaccount.com

Google Cloud Console
============================================================
Setting up NAT Gateway. Use case: instance that dun have external ip but want to access internet
https://cloud.google.com/compute/docs/vpc/special-configurations

Google Cloud Storage
============================================================
# Access control layer (acl)
# https://cloud.google.com/storage/docs/gsutil/commands/acl
# Use -R or -r for applying it recursively
# Use -g id|email to apply to group
gsutil acl ch -R -u john.doe@example.com:W gs://example-bucket
gsutil acl ch -u john.doe@example.com:R gs://example-bucket
gsutil acl ch -u john.doe@example.com:O gs://example-bucket  # Owner
# Set default acl for a bucket for a service account (R, W, O)
gsutil defacl ch -u john@doe.com:R gs://mybucket

# Use multithread (faster when uploading lots of files)
gsutil -m cp -r dir gs://my-bucket

# Set metadata
gsutil -m setmeta -h "Content-Encoding:gzip" gs://....

# To automatically compress files before uploading
# https://cloud.google.com/storage/docs/gsutil/commands/cp
gsutil -m cp -Z myfile gs://bucket/destination/

Adding persistent disk
============================================================

https://cloud.google.com/compute/docs/disks/add-persistent-disk

export DEVICE_ID=sdb
export MOUNT_PATH=/mnt/disks/sdb

sudo lsblk
sudo mkfs.ext4 -m 0 -F -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/$DEVICE_ID
sudo mkdir -p $MOUNT_PATH
sudo mount -o discard,defaults /dev/$DEVICE_ID $MOUNT_PATH
# Grant write access to the device for all users
sudo chmod a+w $MOUNT_PATH

Auto mount
------------------------------------------------------------
sudo cp /etc/fstab /etc/fstab.backup
# Find UUID of the persistent disk
sudo blkid /dev/[DEVICE_ID]
UUID=[UUID_VALUE] [MNT_PATH] ext4 discard,defaults,[NOFAIL] 0 2

# Optionally,
echo UUID=`sudo blkid -s UUID -o value /dev/sdb` /mnt/disks/sdb ext4 discard,defaults,nofail 0 2 | sudo tee -a /etc/fstab

Resizing persistent disk (non-root disk)
============================================================
# Initial check
df -h
sudo lsblk
sudo resize2fs /dev/[DISK_ID]

Common Commands In Compute Engine, GCE
============================================================
# Machine types: f1-micro, n1-standard-1
gcloud compute instances create delete-me-by-$(date -d "+2 days" -I) --machine-type "f1-micro" --image "centos-7-v20170829" --image-project "centos-cloud" --boot-disk-size "20" --boot-disk-type "pd-ssd" --subnet default

gcloud compute instances create delete-me-by-$(date -d "+2 days" -I) --machine-type "f1-micro" --image "ubuntu-1704-zesty-v20170811" --image-project "ubuntu-os-cloud"  --boot-disk-size "20" --boot-disk-type "pd-ssd" --subnet default

# CPU: 1, RAM: 3.75 GB, Disk: 40 GB SSD, CentOS 7
gcloud compute instances create delete-me-by-$(date -d "+2 days" -I) --machine-type "n1-standard-1" --image "centos-7-v20170829" --image-project "centos-cloud" --boot-disk-size "40" --boot-disk-type "pd-ssd" --subnet default

# Delete instances
gcloud compute instances delete [INSTANCE_NAME]
gcloud --project=[MY_GCP_PROJECT] compute instances delete [INSTANCE_NAME]

# List instances
gcloud compute instances list --filter="name~'regex*'"

# List instances in csv format
# https://stackoverflow.com/questions/34600588/i-want-to-use-the-output-of-gcloud-in-a-script-but-the-format-changes-what-s
gcloud compute instances list --format="csv(name,networkInterfaces[0].networkIP:label=INTERNAL_IP,networkInterfaces[0].accessConfigs[0].natIP:label=EXTERNAL_IP,status)"

# SSH into an instance with internal ip
gcloud compute ssh --internal-ip MY_INSTANCE

# Create SSH tunnel
gcloud compute ssh --internal-ip MY_INTANCE -- -NL localhost:8888:localhost:8888

# Get output log from serial port e.g. output from startup script
gcloud compute instances get-serial-port-output [INSTANCE_NAME] \
  --port [PORT] --start [START] --zone [ZONE]

GCE Default Credentials
============================================================
# https://stackoverflow.com/questions/43419575/how-to-access-application-default-credentials-from-a-gce-container-without-any-g
Normally is stored in ~/.config
But when no gcloud commands have been executed, in GCE, it actually resides in instance metadata
# https://cloud.google.com/compute/docs/storing-retrieving-metadata
curl http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token -H "Metadata-Flavor: Google"


Logging
============================================================

Admin Activity
------------------------------------------------------------

# Searching who creates a VM
- First we must retrieve the instance id
  - SSH to the instance then,
    $ curl --header "Metadata-Flavor: Google" http://metadata.google.internal/computeMetadata/v1/instance/id
  - Go the Stackdriver Logging, search in advanced mode:
    resource.labels.instance_id="INSTANCE_ID"


Docker, Image Push, Pull
============================================================
docker tag [IMAGE] [HOSTNAME]/[PROJECT-ID]/[IMAGE][:TAG]

# Docker credential helper
gcloud auth configure-docker

# Alternatively
mkdir ~/.docker
cat <<EOF > ~/.docker/config.json
{
  "credHelpers": {
    "gcr.io": "gcloud", 
    "us.gcr.io": "gcloud", 
    "eu.gcr.io": "gcloud", 
    "asia.gcr.io": "gcloud", 
    "l.gcr.io": "gcloud", 
    "launcher.gcr.io": "gcloud", 
    "us-mirror.gcr.io": "gcloud", 
    "eu-mirror.gcr.io": "gcloud", 
    "asia-mirror.gcr.io": "gcloud", 
    "mirror.gcr.io": "gcloud"
  }
}
EOF

# Alternatively, with "docker-credential-gcr" command
# e.g. when using Google Container Optimized OS
docker-credential-gcr configure-docker

Push
------------------------------------------------------------
gcloud docker -- push [HOSTNAME]/[PROJECT-ID]/[IMAGE]
gcloud docker -- push [HOSTNAME]/[PROJECT-ID]/[IMAGE][:TAG]

Example:
$ docker tag my-image gcr.io/my-project/my-image:test
$ gcloud docker -- push gcr.io/my-project/my-image

Pull
------------------------------------------------------------
gcloud docker -- pull [HOSTNAME]/[PROJECT-ID]/[IMAGE]

Example:
$ gcloud docker -- pull gcr.io/my-project/my-image
$ gcloud docker -- pull gcr.io/my-project/my-image:test


Google API Client
============================================================
Reference:

Compute:
https://cloud.google.com/compute/docs/reference/latest/

Google Cloud PubSub
============================================================
# Start emulator
gcloud beta emulators pubsub start [options]

# Init environment, so app use the emulator vs real one
gcloud beta emulators pubsub env-init
$(gcloud beta emulators pubsub env-init)

# Receive messages
gcloud pubsub subscriptions pull --auto-ack MY_SUBSCRIPTION

git clone --depth=1 https://github.com/GoogleCloudPlatform/python-docs-samples && cd python-docs-samples/pubsub/cloud-client

python publisher.py project1 create topic1
python subscriber.py project1 create topic1 subscription1

python publisher.py project1 publish topic1
python subscriber.py project1 receive subscription1

Google Bigtable
============================================================
https://github.com/spotify/docker-bigtable/issues/2

gcloud beta emulators bigtable start
$(gcloud beta emulators bigtable env-init)

# HBase shell
git clone https://github.com/GoogleCloudPlatform/cloud-bigtable-examples.git
cd cloud-bigtable-examples/quickstart
./quickstart.sh

# cbt command
# Example: cbt -project project1 -instance instance1 createtable table1
gcloud components install cbt

echo project = project1 > ~/.cbtrc
echo instance = instance1 >> ~/.cbtrc

cbt createtable table1
cbt ls 

cbt createfamily table1 cf1
cbt ls table1

cbt set table1 r1 cf1:c1=test-value
cbt read table1

cbt deletetable table1

# In one command
$(gcloud beta emulators bigtable env-init); cbt createtable table1; cbt createfamily table1 cf1; cbt set table1 r1 cf1:c1=test-value; cbt read table1;

# BigQuery extract with compression
bq extract --compression GZIP .....


Cloud Filestore
============================================================
# Mount the fileshare: https://cloud.google.com/filestore/docs/quickstart-gcloud
sudo apt-get -y update
sudo apt-get -y install nfs-common

sudo mkdir /mnt/test
sudo mount 10.0.0.2:/vol1 /mnt/test

Use service account json key when using Docker image google/cloud-sdk
============================================================
export CLOUDSDK_CONFIG=/etc/cloudsdk_config
mkdir -p $CLOUDSDK_CONFIG/configurations
cat <<EOF > $CLOUDSDK_CONFIG/configurations/config_default
[auth]
credential_file_override = /etc/service-accounts/key.json
EOF

Cloud DNS
============================================================
Inbound: Use Google Cloud DNS server to resolve Google's instances and other hostnames
Outbound: Use external DNS server to resolve hostnames from inside Google Cloud

# https://cloud.google.com/dns/zones/#creating_a_dns_policy_that_enables_inbound_dns_forwarding
# Inbound: List proxy that get created for resolving Google internal hostname
gcloud compute addresses list --filter='name ~ ^dns-forwarding.*' \
    --format='csv[no-heading](address, subnetwork)'

# List of API scopes
https://developers.google.com/identity/protocols/googlescopes

# Access scopes for service account in GCE
https://cloud.google.com/sdk/gcloud/reference/alpha/compute/instances/set-scopes#--scopes

# Reserve internal ip
# https://cloud.google.com/compute/docs/ip-addresses/reserve-static-internal-ip-address#reservenewip
gcloud compute addresses create [ADDRESS_NAME] [[ADDRESS_NAME]..] \
    --region [REGION] --subnet [SUBNETWORK] \
    --addresses [IP_ADDRESS]
gcloud compute addresses create example-address-1 \
    --region us-central1 --subnet subnet-1 --addresses 10.128.0.12

Cloud Build
============================================================
gcloud builds submit <docker_build_context> \
    -t gcr.io/<project_id>/<image_name>

# Run executable script in Google Container OS with tmpfs
# https://stackoverflow.com/questions/49037720/cannot-run-executable-shell-script-on-google-container-optimized-os
sudo mkdir /mnt/disks/scratch
sudo mount -t tmpfs tmpfs /mnt/disks/scratch/