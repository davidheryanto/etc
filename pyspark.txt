# Example for common way to create PySpark session

from pyspark.sql import SparkSession
spark = (
    SparkSession.builder
    .config("spark.sql.execution.arrow.pyspark.enabled", "true")
    .config("spark.driver.memory", "16g")
    .config("spark.executor.memory", "8g")
    .config("spark.driver.maxResultSize", "8g")
    .config("spark.sql.parquet.int96RebaseModeInRead", "legacy")
    .getOrCreate()
)

# Add packages from maven repository, comma separated packages
spark = (
    SparkSession.builder
    .config("spark.jars.packages", "org.apache.hadoop:hadoop-azure:3.0.3,io.delta:delta-spark_2.12:3.2.0")
    .getOrCreate()
)

# Disable logging in Jupyter notebook
# https://stackoverflow.com/a/70613254
#
# Edit ~/.ipython/profile_default/ipython_kernel_config.py
c.IPKernelApp.capture_fd_output = False
#
# For setting it only for the particular kernel
# $ jupyter kernelspec list, then go to the kernel dir, find kernel.json
# Add this after ipykernel launcher, before -f
"--IPKernelApp.capture_fd_output=False"

# Getting started locally in standalone mode
# ============================================================
# https://spark.apache.org/downloads.html
#
# Example using PySpark:
# - pip install pyspark

wget https://apachemirror.sg.wuchna.com/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz
tar xvf spark-3.0.1-bin-hadoop2.7.tgz
cd spark-3.0.1-bin-hadoop2.7
./sbin/start-master.sh

python <<EOF

import pyspark
import random

num_samples = 100000000

def inside(p):
    x, y = random.random(), random.random()
    return x*x + y*y < 1

sc = pyspark.SparkContext(appName="Pi")
count = sc.parallelize(range(0, num_samples)).filter(inside).count()
pi = 4 * count / num_samples

print(pi)

EOF

./sbin/stop-master.sh

# Print configuration from SparkContext, sc variable below
# https://stackoverflow.com/a/56778417
sc.getConf().getAll()

# Common environment variables for running pyspark
HADOOP_CONF_DIR=/etc/hadoop/conf
SPARK_HOME=/usr/lib/spark
PYSPARK_PYTHON=/home/user/miniconda3/bin/python

# Increase no of executors by updating spark-env.sh
SPARK_EXECUTOR_CORES=4

# Sample codes to test
# ============================================================
import random
num_samples = 100000000

def inside(p):
    x, y = random.random(), random.random()
    return x*x + y*y < 1


count = sc.parallelize(range(0, num_samples)).filter(inside).count()
pi = 4 * count / num_samples
print(pi)

# Compared to native Python
pi = 4 * sum([inside(x) for x in range(0, num_samples)]) / num_samples
print(pi)
# ============================================================

# PySpark with BigQuery connector
# https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example

from pyspark.sql import SparkSession

spark = SparkSession \
  .builder \
  .master('yarn') \
  .appName('spark-bigquery-demo') \
  .getOrCreate()

# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector
bucket = "[bucket]"
spark.conf.set('temporaryGcsBucket', bucket)

# Create Spark Session and load BigQuery jars
# https://github.com/GoogleCloudDataproc/spark-bigquery-connector/issues/81

from pyspark.sql import SparkSession

spark = SparkSession.builder \
  .appName("spark-shell-bq")\
  .config("spark.jars", "gs://spark-lib/bigquery/spark-bigquery-latest.jar") \
  .getOrCreate()

# Load data from BigQuery.
words = spark.read.format("bigquery") \
  .option("table", "bigquery-public-data:samples.shakespeare") \
  .load()
words.createOrReplaceTempView("words")

# Perform word count.
word_count = spark.sql(
    "SELECT word, SUM(word_count) AS word_count FROM words GROUP BY word")
word_count.show()
word_count.printSchema()

# Saving the data to BigQuery
word_count.write.format("bigquery") \
  .option("table", "wordcount_dataset.wordcount_output") \
  .save()

# Read Delta Table
# https://docs.delta.io/latest/quick-start.html

from pyspark.sql import SparkSession
spark = (
    SparkSession.builder
    .config("spark.jars.packages", "io.delta:delta-core_2.12:2.1.0") 
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
    .getOrCreate()
)
spark.read.format("delta").load("/path/to/delta/table")

# Spark set driver memory
spark = (
    SparkSession.builder
    .config("spark.driver.memory", "8g")
    .getOrCreate()
)

# Optimize conversion between Spark and Pandas DataFrame
# https://docs.databricks.com/spark/latest/spark-sql/spark-pandas.html
spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "true")