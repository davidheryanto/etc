# Print configuration from SparkContext, sc variable below
# https://stackoverflow.com/a/56778417
sc.getConf().getAll()

# Common environment variables for running pyspark
HADOOP_CONF_DIR=/etc/hadoop/conf
SPARK_HOME=/usr/lib/spark
PYSPARK_PYTHON=/home/user/miniconda3/bin/python

# Increase no of executors by updating spark-env.sh
SPARK_EXECUTOR_CORES=4

# Sample codes to test
# ============================================================
import random
NUM_SAMPLES = int(3e8)

def inside(p):
    x, y = random.random(), random.random()
    return x*x + y*y < 1


count = sc.parallelize(range(0, NUM_SAMPLES)).filter(inside).count()
pi = 4 * count / NUM_SAMPLES
print(pi)

# Compared to native Python
pi = 4 * sum([inside(x) for x in range(0, NUM_SAMPLES)]) / NUM_SAMPLES
print(pi)
# ============================================================

# PySpark with BigQuery connector
# https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example

from pyspark.sql import SparkSession

spark = SparkSession \
  .builder \
  .master('yarn') \
  .appName('spark-bigquery-demo') \
  .getOrCreate()

# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector
bucket = "[bucket]"
spark.conf.set('temporaryGcsBucket', bucket)

# Create Spark Session and load BigQuery jars
# https://github.com/GoogleCloudDataproc/spark-bigquery-connector/issues/81

from pyspark.sql import SparkSession

spark = SparkSession.builder \
  .appName("spark-shell-bq")\
  .config("spark.jars", "gs://spark-lib/bigquery/spark-bigquery-latest.jar") \
  .getOrCreate()

# Load data from BigQuery.
words = spark.read.format("bigquery") \
  .option("table", "bigquery-public-data:samples.shakespeare") \
  .load()
words.createOrReplaceTempView("words")

# Perform word count.
word_count = spark.sql(
    "SELECT word, SUM(word_count) AS word_count FROM words GROUP BY word")
word_count.show()
word_count.printSchema()

# Saving the data to BigQuery
word_count.write.format("bigquery") \
  .option("table", "wordcount_dataset.wordcount_output") \
  .save()
