CLI: https://cloud.google.com/bigquery/docs/reference/bq-cli-reference
============================================================
# Query
bq query --use_legacy_sql=false -n 5 [QUERY]

# Query Example
bq query --nouse_legacy_sql \
'SELECT
   COUNT(*)
 FROM
   `bigquery-public-data`.samples.shakespeare'

# Delete table
bq rm -f -t [data_set.table_name]

# Create table
bq mk \
--table \
--expiration 3600 \
--description "This is my table" \
--label environment:test \
mydataset.mytable \
name:STRING,count:INT64,floatval:FLOAT

=============================================================

# Partitioned Tables
# - So that we don't need to look at the WHOLE table
# - We can reference it from legacy SQL as dataset.table$20160519
# https://cloud.google.com/bigquery/docs/partitioned-tables

SELECT
  _PARTITIONTIME AS pt,
FROM
  [dataset.table]


# Insert data and use column data as partition value
# Not supported yet, but can insert into specific table partition
# https://stackoverflow.com/questions/38878914/partitioning-based-on-column-data

When inserting from the UI, specify target table as dataset.table$20170101

# Check table schema and partition i.e. describe table schema
bq show PROJECT:DATASET.TABLE

# Sample ~/.bigqueryrc
project_id=GOOGLE_PROJECT

[query]
--use_legacy_sql=false

# Creating an empty table
# https://cloud.google.com/bigquery/docs/tables
bq mk \
--table \
--expiration 3600 \
--description "This is my table" \
--label organization:development \
myproject:mydataset.mytable \
qtr:STRING,sales:FLOAT,year:STRING

# Load csv from local file
bq load \
--source_format=CSV \
myproject:mydataset.mytable \
./mydata.csv \
qtr:STRING,sales:FLOAT,year:STRING

# List jobs 
bq ls --jobs --max_results=10

# Example of creating test data to test functions
# https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators#lead
WITH
  t AS (
  SELECT
    1234.56 AS val
  UNION ALL SELECT 98.12
  UNION ALL SELECT 9237851 )
SELECT
  FORMAT("%'.0f", val) val,
FROM
  t

# After exporting BigQuery Audit Logs, query the dollar spent by users
# https://dataform.co/blog/exporting-bigquery-usage-logs
SELECT
  protopayload_auditlog.authenticationInfo.principalEmail,
  SUM(5.0 * CAST( json_extract_scalar( protopayload_auditlog.metadataJson,
        "$.jobChange.job.jobStats.queryStats.totalBilledBytes" ) AS int64 )) AS spend
FROM
  `PROJECT_ID.DATASET.cloudaudit_googleapis_com_data_access`
WHERE
  resource.type = 'bigquery_project'
  AND DATE(timestamp) = DATE(TIMESTAMP_SUB(current_timestamp, INTERVAL 1 day))
GROUP BY
  1
ORDER BY
  2 DESC

# List all partitions
SELECT _PARTITIONTIME as pt, FORMAT_TIMESTAMP("%Y%m%d", _PARTITIONTIME) as partition_id
FROM `PROJECT_ID.DATASET_ID.TABLE_ID`
GROUP BY _PARTITIONTIME
ORDER BY _PARTITIONTIME

# Example UNNEST with ARRAY of STRUCT
SELECT
  *
FROM
  UNNEST(ARRAY<STRUCT<x STRING, y STRING>>[("x1",
      'y1'), ("x2",
      'y2')])
      LEFT JOIN
  UNNEST(ARRAY<STRUCT<foo STRING, bar STRING>>[("foo1",
      'bar1'), ("foo2",
      'bar2')])