===========
Extra Info
===========
https://deis.com/blog/2016/kubernetes-illustrated-guide/

Good links about Kubernetes networking:
https://cloud.google.com/solutions/prep-container-engine-for-prod

App inside pod, authentication to Google Cloud
https://cloud.google.com/kubernetes-engine/docs/tutorials/authenticating-to-cloud-platform

# Check latest stable version
curl -L https://storage.googleapis.com/kubernetes-release/release/stable.txt

# Download CLI specific version
curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.16.0/bin/linux/amd64/kubectl

============================================================
					    Cheat Sheet
https://kubernetes.io/docs/user-guide/kubectl-cheatsheet/
============================================================

kubectl create -f ./my1.yaml -f ./my2.yaml
kubectl create -f ./dir

============================================================

minikube version
minikube start

kubectl version
kubectl cluster-info
kubectl get nodes

kubectl run
kubectl run kubernetes-bootcamp --image=docker.io/jocatalin/kubernetes-bootcamp:v1 --port=8080
kubectl run ${DEPLOYMENT_NAME} --image=debian:stretch -i --tty
kubectl run nginx-deployment -it --image=nginx --port=80
kubectl run nginx-deployment -l 'app=myapp,key=value'
kubectl get deployments

# filter resources retrived by metadata name
kubectl get deployments --field-selector metadata.name=MY_DEPLOYMENT

# Use kubectl run to create and run pod
kubectl run python --image=python --generator=run-pod/v1 --command -- tail -f /dev/null
kubectl wait --for=condition=Ready pod/python

kubectl run debian --image=debian --generator=run-pod/v1 --command -- tail -f /dev/null
kubectl run debian --image=debian --generator=run-pod/v1 --requests='cpu=1,memory=256Mi' --command -- tail -f /dev/null
kubectl run debian --image=debian --generator=run-pod/v1 --command -- sh -c "sleep infinity"
kubectl run myapp  --image=debian --generator=run-pod/v1 --labels app=myapp --command -- sh -c "sleep infinity"

# Expose the pod as a service
kubectl expose pod myapp --port=80 --name=myapp

# To check the generated yaml before running the command, use: -o yaml --dry-run
kubectl run [NAME] --generator=run-pod/v1 --image=google/cloud-sdk:slim -o yaml --dry-run  --command -- sh -c "echo Hello"

# Filter retrieved pods by status, e.g. hide "completed" pods
kubectl get -n airflow --field-selector=status.phase!=Succeeded pod

kubectl proxy

kubectl get pods
# Show more info for pods, e.g. the IP address
kubectl get pods -o wide
kubectl describe pods

# Get pod by label, sort by created, retrieve the name only
# https://kubernetes.io/docs/reference/kubectl/jsonpath/
kubectl get pod -l app=myapp1 --sort-by=.metadata.creationTimestamp \
-o jsonpath='{.items[0].metadata.name}'

get pod -l app=app_name -o jsonpath={.items[0].metadata.name} \
--field-selector=status.phase!=Running

# Filter by selector, e.g. there's spec.selector.component: myapp
kubectl get svc -l component=myapp

# Filter with jsonpath: https://kubernetes.io/docs/reference/kubectl/jsonpath/
kubectl get pods -o=jsonpath='{@}'
# e.g. get the NodePort
kubectl get svc -l component=myapp -o jsonpath='{.items[1].spec.ports[0].nodePort}'

# Cheatsheet: https://github.com/fabric8io/kansible/blob/master/vendor/k8s.io/kubernetes/docs/user-guide/kubectl-cheatsheet.md

kubectl logs $POD_NAME

kubectl exec $POD_NAME env
kubectl exec -ti $POD_NAME bash

# kubectl exec if more than one container in a pod
kubectl exec -it my-pod --container main-app -- /bin/bash

# Copy directory from local to pod
kubectl cp /tmp/foo_dir <some-pod>:/tmp/bar_dir

kubectl config view
kubectl config get-clusters
kubectl config get-contexts

# Get all the context name
kubectl config -o json view | jq '.contexts[].name'
kubectl config get-contexts -o name

# Cleanup contexts, assuming /tmp/contexts contains newline separated context names to delete
while read context; do kubectl config delete-context $context; done < /tmp/contexts

# Context: list, check current, switch
# Config location: ~/.kube/config
kubectl config view | grep -A1 " context:"
kubectl config view -o jsonpath='{.contexts[*].name}'

kubectl config current-context
kubectl config use-context CONTEXT_NAME

kubectl get namespaces --show-labels

kubectl port-forward POD_NAME 8080:8080

# When port-forward kept having timeouts, we can retry if error
while true; sleep 1; do kubectl --namespace argo port-forward POD_NAME LOCAL_PORT:REMOTE_PORT; done

# Force delete pod: https://stackoverflow.com/questions/35453792/pods-stuck-at-terminating-status
kubect delete pod [POD_NAME] --grace-period=0 --force

# Delete pod by selector: delete "completed" pods
# https://stackoverflow.com/a/57750773
kubectl delete pod --field-selector=status.phase==Succeeded

# View pod events
kubectl describe pod POD_NAME

# Check permission, impersonate user/service accounts
# https://kubernetes.io/docs/reference/access-authn-authz/authentication/#user-impersonation
kubectl auth can-i --as system:serviceaccount:default:default --list
kubectl auth can-i --as system:serviceaccount:default:default '*' '*'  # cluster-admin clusterrole
kubectl auth can-i --as system:serviceaccount:mynamespace:default get pod
kubectl auth can-i --as system:serviceaccount:default:default get /logs/

# Check status for cluster auto-scaler e.g. why node pools not
# scaling up: https://stackoverflow.com/q/50872284
kubectl describe -n kube-system configmap cluster-autoscaler-status

# Wait for a specific condition on one or many resources
kubectl wait --for=condition=Ready pod/busybox1
kubectl wait --for=delete pod/busybox1 --timeout=60s

Secrets & Configmaps
----------------------
kubectl create secret generic SECRET_NAME --from-file=SECRET_FILEPATH
kubectl create secret generic SECRET_NAME --from-literal=username=USERNAME --from-literal=password=PASSWORD

kubectl create secret generic tls-certs --from-file=tls/
kubectl describe secrets tls-certs  # Verify

# Decode secret with `base64 --decode`
# Assuming secret data is a json with field called MYDATA
kubectl get secret SECRET_NAME -o json | jq --raw-output .data.MYDATA | base64 --decode

kubectl create configmap nginx-proxy-conf --from-file=nginx/proxy.conf
kubectl describe configmap nginx-proxy-conf  # More details

# Get yaml representation of current config file
kubectl get configmap CONFIG_NAME -o yaml

# Get yaml file from configmap
kubectl get configmap CONFIG_NAME -o json | jq --raw-output '.data["myapp.conf"]'

# Update configmap
# https://stackoverflow.com/questions/38216278/update-k8s-configmap-or-secret-without-deleting-the-existing-one
kubectl create configmap CONFIGMAP_NAME --from-file FILE_PATH -o yaml --dry-run | kubectl apply -f -

# Get the yaml that will produce existing deployment
# https://github.com/kubernetes/kubernetes/issues/24873
kubectl get deploy [DEPLOYMENT_NAME] -o yaml --export
# Think for boolean parameter it's optional to provide truthy argument
kubectl get deploy [DEPLOYMENT_NAME] -o yaml --export=true  

# Get the yaml that produce existing cluster
kubectl get all --export -o yaml

# Easily see the roles or binding assigned to a user or service account, rbac
https://github.com/FairwindsOps/rbac-lookup

Setting namespace to config
------------------------------------------------------------

https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/

kubectl config set-context $(kubectl config current-context) --namespace=<insert-namespace-name-here>
# Validate it
kubectl config view | grep namespace:

# Alternatively, vim ~/.kube/config
- context:
    cluster: CLUSTER_NAME
    namespace: SET_YOUR_DEFAULT_NAMESPACE


Flow
============================================================
kubectl delete deployments/DEPLOYMENT_NAME services/SERVICE_NAME
kubectl create -f DEPLOYMENT_SERVICE_YAML

Deployment 
============================================================
# For initial deployment and UPDATE in deployment
kubectl apply -f deployment.yaml

# Delete deployment by name
kubectl get deployments  | grep ${DEPLOYMENT_NAME} | awk '{print $1}' | xargs kubectl delete deployments

Services
=========================================
- Match by labels and selectors
  e.g. 
  - dev, test, production labels
  - version, tags

kubectl get services
kubectl expose deployment/kubernetes-bootcamp --type="NodePort" --port 8080
kubectl describe services/kubernetes-bootcamp

kubectl expose deployment hello-node --type=LoadBalancer
kubectl get services
# To access on minikube: minikube service hello-node

Using labels
------------
kubectl get deployments
kubectl describe deployment
kubectl get pods -l run=kubernetes-bootcamp
kubectl get services -l run=kubernetes-bootcamp

Applying label
--------------
kubectl label pod $POD_NAME app=v1
kubectl get pods -l app=v1

Deleting services
=========================================
kubectl delete service -l run=kubernetes-bootcamp
kubectl get services

Scaling
============================================================
- By setting no of replicas

# Check the desired, current, available pods
kubectl get deployments

kubectl scale deployments/kubernetes-bootcamp --replicas=4

kubectl get pods -o wide
kubectl describe deployments/kubernetes-bootcamp

Load balancing
--------------
kubectl describe services/kubernetes-bootcamp
# Can check that requests to $HOST:$PORT are handled by different pods

Updating
============================================================
Rolling updates
- Update an application (container image update)
- Rollback
- CI and CD (zero downtime)

kubectl get deployments
kubectl get pods
kubectl describe pods

kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2

# Rolling restart a deployment
# https://kubernetes.io/docs/reference/kubectl/cheatsheet/#updating-resources
kubectl rollout restart deployment/frontend

kubectl get pods  # See update status
curl $HOST:$PORT  # Check that it's new version
kubectl rollout status deployments/kubernetes-bootcamp  # Alternatively, to check

# Show diff between deployment revisions
diff <(kubectl rollout history deployment/[MY_DEPLOYMENT] --revision=1) <(kubectl rollout history deployment/[MY_DEPLOYMENT] --revision=2)

# Show diff between helm releases
# https://github.com/databus23/helm-diff/issues/6
sdiff -s -w 80 <(helm get $RELEASE --revision $NEW_REVISION) <(helm get $RELEASE --revision $OLD_REVISION)

Rolling back
------------
kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v10  # Update to non-existent version
kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v1  # Rollback

Logging
============================================================
kubectl logs [-f] $POD_NAME


Deleting
============================================================
kubectl delete service hello-node
kubectl delete deployment hello-node

# Delete all pods in a namespace 
# https://stackoverflow.com/questions/33509194/command-to-delete-all-pods-in-all-kubernetes-namespaces
kubectl delete --all pods --namespace=foo

Cleanup
-------
kubectl delete pods healthy-monolith monolith secure-monolith
kubectl delete services monolith auth frontend hello
kubectl delete deployments auth frontend hello
kubectl delete secrets tls-certs
kubectl delete configmaps nginx-frontend-conf nginx-proxy-conf


============================================================
				   Minikube
============================================================
eval $(minikube docker-env)  # Use docker daemon
eval $(minikube docker-env -u)  # Stop using


============================================================
						Udacity
============================================================

Provision kubernetes cluster
----------------------------
gcloud container clusters create k0 

kubectl create -f pods/monolith.yaml
kubectl get pods
kubectl describe pods monolith

kubectl port-forward monolith 10080:80
kubectl logs monolith
kubectl exec monolith --stdin --tty -c monolith /bin/sh

- Readiness
- Liveness: restart container

  readinessProbe:
    httpGet:
      path: /readiness
      port: 81
      scheme: HTTP
    initialDelaySeconds: 5
    timeoutSeconds: 1

  livenessProbe:
    httpGet:
      path: /healthz
      port: 81
      scheme: HTTP
    initialDelaySeconds: 5
    periodSeconds: 15
    timeoutSeconds: 5


Service discovery
------------------
spec:
  selector:
    app: "monolith"
    secure: "enabled"
  ports:
  - protocol: "TCP"
    port: 443
    targetPort: 443
    nodePort: 31000
  type: NodePort

kubectl create -f services/monolith.yaml 
gcloud compute firewall-rules create allow-monolith-nodeport --allow=tcp:31000
gcloud compute instances list

# Troubleshoot endpoints not available
kubectl get pods -l "app=monolith"
kubectl get pods -l "app=monolith,secure=enabled"

# Check pod label
kubectl describe pods secure-monolith | grep Labels
# Add pod label
kubectl label pods secure-monolith "secure=enabled"

Deployment
----------
kubectl create -f deployments/hello.yaml

kubectl create -f deployments/auth.yaml
kubectl describe deployments auth
kubectl create -f services/auth.yaml
kubectl create -f services/hello.yaml

kubectl create configmap nginx-frontend-conf --from-file=nginx/frontend.conf
kubectl create -f deployments/frontend.yaml 
kubectl create -f services/frontend.yaml
kubectl get service frontend

# Remember to open port at the external firewall

Scaling
-------
kubectl get replicasets
vim deployments/hello.yaml  # Increment value of replicas
kubectl apply -f deployments/hello.yaml
kubectl get replicasets

Rolling update
--------------
vim deployments/auth.yaml  # Update image su version 2.0.0
kubectl apply -f deployments/auth.yaml

# GKE warning when using multiple ingress controllers
# https://support.cloudbees.com/hc/en-us/articles/360019569372-GKE-Warning-when-using-multiple-Ingress-controllers
# Error during sync: error while evaluating the ingress spec: service "<namespace>/<masterName>" is type "ClusterIP", expected "NodePort" or "LoadBalancer"
# Fix it by adding annotation to the Ingress resource
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: "nginx"

Adding secrets
------------------------------------------------------------
# For imagePullSecrets in pod.yaml
kubectl create secret docker-registry image-pull-secret --docker-server "https://asia.gcr.io" --docker-username _json_key --docker-email not@val.id --docker-password="$(cat service-account-one-liner.json)"


RBAC
============================================================
Set user as cluster-admin
https://github.com/kubernetes/ingress-nginx/issues/1663

GCLOUD_USER=$(gcloud config get-value account | cut -d@ -f1)

kubectl create clusterrolebinding ${GCLOUD_USER}-cluster-admin-binding \
  --clusterrole cluster-admin \
  --user $(gcloud config get-value account)

# Give cluster-admin role to service account "default" in namespace "default"
kubectl create clusterrolebinding default-cluster-admin-binding \
  --clusterrole cluster-admin \
  --serviceaccount=default:default

Bug
============================================================
https://github.com/kubernetes/dashboard/wiki/FAQ#i-am-seeing-404-errors-when-trying-to-access-dashbord-dashboard-resources-can-not-be-loaded

Error:
- clusterroles.rbac.authorization.k8s.io "USER" is forbidden: attempt to grant extra privileges
Solution:
- https://stackoverflow.com/questions/45281998/istio-installation-on-gke-failed-with-clusterroles-rbac-authorization-k8s-io-i
- Not sure why it works

Sample deployment.yaml
============================================================
apiVersion: apps/v1 
kind: Deployment
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    spec:
      initContainers:
      - name: init-sysctl
        image: busybox:1.27.2
        # To use image hash / digest
        # image: busybox@sha256:xxxxxxxxxxxxxxxxxxxxxxx
        command:
        - sysctl
        - -w
        - vm.max_map_count=262144
        securityContext:
          privileged: true

      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
        env:
        - name: KEY
          value: "My value"
        - name: SECRET_USERNAME
          valueFrom:
            secretKeyRef:
              name: mysecret
              key: username
--- 
kind: Service
apiVersion: v1
metadata:
  [namespace: demo]
  name: nginx
  labels:
    app: nginx
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 80
    protocol: TCP
  selector:
    app: nginx

============================================================
Sample pod.yaml with configmap
============================================================
apiVersion: v1
kind: Pod
metadata:
  name: deleteme
  labels:
    env: test
spec:
  containers:
  - name: deleteme
    image: debian:stretch
    command: ["sh", "-c", "sleep infinity"]
    env:
    - name: SECRET_USERNAME
      valueFrom:
        secretKeyRef:
          name: mysecret
          key: username
    volumeMounts:
    - name: config-volume
      mountPath: /etc/config
    - name: secret-volume
      mountPath: "/path/to/secret"
      readOnly: true
    - name: cache-volume
      mountPath: /cache
    resources:
      requests:
        memory: 64Mi
        cpu: 250m
      limits:
        memory: 128Mi
        cpu: 500m
  volumes:
  - name: config-volume
    configMap:
      name: deleteme
  - name: secret-volume
    secret:
      secretName: mysecret
  - name: cache-volume
    emptyDir: {}

# How to mount a single file 
# https://github.com/kubernetes/kubernetes/issues/44815#issuecomment-297077509

containers:
- volumeMounts:
  - name: demo-config
    mountPath: /app/settings.json
    subPath: settings.json
volumes:
- name: demo-config
  configMap:
    name: demo

============================================================
Sample pod.yaml with PVC (persistent volume claim)
============================================================

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 30Gi
---
kind: Pod
apiVersion: v1
metadata:
  name: task-pv-pod
spec:
  volumes:
  - name: task-pv-storage
    persistentVolumeClaim:
     claimName: task-pv-claim
  containers:
  - name: task-pv-container
    image: nginx
    ports:
    - containerPort: 80
      name: "http-server"
    volumeMounts:
    - mountPath: "/usr/share/nginx/html"
      name: task-pv-storage


Internal Load Balancer
============================================================
apiVersion: v1
kind: Service
metadata:
  name: [SERVICE_NAME]
  annotations:
    cloud.google.com/load-balancer-type: "Internal"
  labels:
    [KEY]: [VALUE]
spec:
  type: LoadBalancer
  loadBalancerIP: [IP_ADDRESS] # if omitted, an IP is generated
  loadBalancerSourceRanges: [IP_RANGE] # defaults to 0.0.0.0/0
  ports:
  - name: [PORT_NAME]
    port: 80
    targetPort: 8080
    protocol: TCP # default; can also specify UDP
  selector:
    [KEY]: [VALUE] # label selector for Pods to target

# Clean up Kubernetes jobs
# https://stackoverflow.com/questions/43675231/kubernetes-delete-all-jobs-in-bulk
for j in $(kubectl get jobs -o custom-columns=:.metadata.name)
do
    kubectl delete jobs $j &
done

# kubectl apply from stdin 
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: kafka-test-client
spec:
  containers:
  - name: kafka
    image: confluentinc/cp-kafka:4.1.2-2
    command:
    - sh
    - -c
    - exec tail -f /dev/null
EOF

# tcpdump to check network traffic on pods: https://github.com/eldadru/ksniff
# Install ksniff, make sure the binary is in the PATH
kubectl sniff <POD_NAME> [-n <NAMESPACE_NAME>] [-c <CONTAINER_NAME>] [-i <INTERFACE_NAME>] [-f <CAPTURE_FILTER>] [-o OUTPUT_FILE] [-l LOCAL_TCPDUMP_FILE] [-r REMOTE_TCPDUMP_FILE]

# Usually the container being debugged runs as non-privileged, can use -p privileged 
kubectl sniff -p <POD_NAME> -n <NAMESPACE>

# Stream the stdout, make tshark (from Wireshark) is installed
kubectl sniff -p <POD_NAME> -n <NAMESPACE> -o - | tshark -r -

# tcpdump filter for only HTTP POST request port 8080
kubectl sniff -p <POD_NAME> -n <NAMESPACE> -f 'tcp dst port 8080 and (tcp[((tcp[12:1] & 0xf0) >> 2):4] = 0x504f5354)' -o - | tshark -r -

# Manually delete nodes in GKE
# When cluster autoscaler cannot scale down nodes because something is preventing it
#
# https://github.com/kubernetes/autoscaler/blob/cluster-autoscaler-1.2.2/cluster-autoscaler/FAQ.md#what-types-of-pods-can-prevent-ca-from-removing-a-node
# https://stackoverflow.com/a/43098104/3949303
#
kubectl cordon NODE
kubectl drain  NODE --ignore-daemonsets --force
gcloud compute instance-groups managed delete-instances \
  gke-cluster-default-pool-9cc4e660-grp \
  --instances=gke-jcluster-default-pool-9cc4e660-rx9p \
  --zone=...

# Using volume emptyDir can prevent pods from being evicted.
# Hence, preventing cluster autoscaler to scale down node groups.
# To indicate that pods using emptyDir can be evicted, use this annotation.
kind: Pod
metadata:
  name: annotations-demo
  annotations:
    cluster-autoscaler.kubernetes.io/safe-to-evict: "true"

# Using default docker registry prefix e.g. for nginx
registry.hub.docker.com/library/nginx:latest

# Zero downtime rolling updates of deployment
# https://blog.sebastian-daschner.com/entries/zero-downtime-updates-kubernetes
# https://azure.github.io/application-gateway-kubernetes-ingress/how-tos/minimize-downtime-during-deployments/
kind: Deployment
spec:
  template:
    spec:
      terminationGracePeriodSeconds: 50
      containers:
      - name: mycontainer
        lifecycle:
          preStop:
            exec:
              command: ["/bin/bash", "-c", "sleep 45"]