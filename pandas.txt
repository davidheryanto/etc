# Count NaN values in each column
df.isnull().sum()
# isnull opposite is notnull

# Check if in a DataFrame any row is NaN
df[df.isnull().any(axis=1)].head()

# Replace missing data with mean
# http://stackoverflow.com/questions/18689823/pandas-dataframe-replace-nan-values-with-average-of-columns
df.fillna(df.mean())

# Count unique values 
nunique()

# MultiIndex selection
A.loc[first_index_val, second_index_val]

# Flatten multiindex into columns
# http://stackoverflow.com/questions/14507794/python-pandas-how-to-flatten-a-hierarchical-index-in-columns
DataFrame(df.to_records())

# Select columns by regex: ^ starts with, $ ends with
df.filter(regex='AMOUNT|^PRICE|QUANTITY$').head()

# Print all columns
# Make it max: 'display.max_rows', None 
with pd.option_context('display.max_rows', 999, 'display.max_columns', 3):
    print df
# Permanently
pd.set_option('display.max_columns', None, 'display.max_rows', 500)
# Reset 
pd.reset_option('display.max_rows')

# Print df in table form
from IPython.display import display
with pd.option_context('display.max_rows', 999, 'display.max_columns', 3):
    display(df)

# Find row where values for column is maximal
# http://stackoverflow.com/questions/10202570/pandas-dataframe-find-row-where-values-for-column-is-maximal
df = pandas.DataFrame(np.random.randn(5,3),columns=['A','B','C'])
	      A         B         C
	0  1.232853 -1.979459 -0.573626
	1  0.140767  0.394940  1.068890
	2  0.742023  1.343977 -0.579745
	3  2.125299 -0.649328 -0.211692
	4 -0.187253  1.908618 -1.862934
# Get row where value for col A is max
dfrm.ix[dfrm['A'].idxmax()]

# Calculate difference
df['first_diff'] = df.A - df.A.shift(1)

# Create empty dataframes to be appended to existing ones
from dateutil.relativedelta import relativedelta

start = datetime.datetime.strptime("1982-07-01", "%Y-%m-%d")
date_list = [start + relativedelta(months=x) for x in range(0,12)]

future = pd.DataFrame(index=date_list, columns=df.columns)
df = pd.concat([df, future])

# Get counts of distinct values in all columns
df.value_counts()

# Drop columns
df.drop(['col1', 'col2'], axis=1, inplace=True)

# Reset index, e.g. after drop_na
.reset_index(drop=True)  # drop=True prevents creation of new index col

# Set index using column(s)
# http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html
# keys : column label or list of column labels / arrays
# drop : boolean, default True | Delete columns to be used as the new index
DataFrame.set_index(keys, drop=True, append=False, inplace=False, verify_integrity=False)[source]

# Create new column based on if-else
# http://stackoverflow.com/questions/21702342/creating-a-new-column-based-on-if-elif-else-condition
def f(row):
    if row['A'] == row['B']:
        val = 0
    elif row['A'] > row['B']:
        val = 1
    else:
        val = -1
    return val
df['C'] = df.apply(f, axis=1)

# Pandas assign group id
# http://stackoverflow.com/questions/15072626/get-group-id-back-into-pandas-dataframe
df["GroupId"] = df.groupby(["Name", "Rank"]).grouper.group_info[0]

# Group by then create a new column result
A = DataFrame({'a': [1,3,4,1], 'b': [1,2,5,9]})
def f(x):
    x['c'] = x['b'].sum()
    return x
A.groupby('a').apply(f)

# Modify multiple columns in a row with apply and axis=1
def update_row(row):
    row.col_1 = row.col_2 + ' modified'
    row.col_3 = 'modified ' + row.col_5
    return row  # Important

df.apply(update_row, axis=1)

# Filter grouped values
filtered_po = po.groupby('PO No.')['PO No.'].filter(lambda x: len(x) > 1)
# Now select intersect this with the original PO
po[po['PO No.'].isin(filtered_po)]

# Group by (year, month)
# http://stackoverflow.com/questions/24082784/pandas-dataframe-groupby-datetime-month
df.groupby(pd.TimeGrouper(freq='M'))

# Group by month, the column must be of datetime type
# http://stackoverflow.com/questions/26646191/pandas-groupby-month-and-year
B = A.set_index('date')
B.groupby(D.index.month).size()

# Sort values by multiple columns
df.sort_values(['a', 'b'], ascending=[True, False])

# Find days diff: http://stackoverflow.com/questions/16103238/pandas-timedelta-in-days
(df['today'] - df['date']).dt.days

# Find hours diff
# http://stackoverflow.com/questions/31283001/get-total-number-of-hours-from-a-pandas-timedelta
td = pd.Timedelta('1 days 2 hours')
td / np.timedelta64(1, 'h')

# Convert datetime to unix timestamp and vice versa
# http://stackoverflow.com/questions/15203623/convert-pandas-datetimeindex-to-unix-time
# http://stackoverflow.com/questions/16517240/pandas-using-unix-epoch-timestamp-as-datetime-index
df.astype(np.int64) // 10**9
df.astype('datetime64[s]')

# Combine / Concat columns of strings
# http://stackoverflow.com/questions/19377969/combine-two-columns-of-text-in-dataframe-in-pandas-python
df['period'] = df[['Year', 'quarter']].apply(lambda x: ''.join(x), axis=1)
# If some column is not str type
df['period'] = df[['Year', 'quarter']].astype(str).apply(lambda x: ''.join(x), axis=1)

# Create sample DataFrame
A = DataFrame({'A': np.random.randint(1,4,10), 'B': np.random.randint(2,10,10), 'C': np.random.randint(5,8,10)})

# Group by then apply function to multiple column in a group
# Key: After groupby, immediately call apply (instead of selection the columns)
#      Because then the function will be applied on each column
def f(x):
    _range = x['B'].max() - x['B'].min()
    return 1.0 * x['A'].sum() / _range
A.groupby('A').apply(f)

# Transform from long to wide format
df.pivot_table(values=column, index=[columns], columns=[columns], aggfunc=f, fill_value=0, dropna)

# Transform from wide to long format
pd.melt(df, id_vars, value_vars, var_name, value_name)

Plot correlation
============================================================
import seaborn as sns
import matplotlib.pyplot as plt
sns.set(context="paper", font="monospace")

# Load the datset of correlations between cortical brain networks
df = qc[['err_class'] + qc.columns.tolist()[2:]]
corrmat = df.corr()

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(16, 12))

# Draw the heatmap using seaborn
sns.heatmap(corrmat, vmax=.8, square=True)

f.tight_layout()
f.savefig('corr_heatmap_4.png', bbox_inches='tight')


Essentials
============================================================
# Reindexing
obj = Series([4.5, 7.2, -5.3, 3.6], index=['d', 'b', 'a', 'c'])
obj.reindex(['a', 'b', 'c', 'd', 'e'])
obj.reindex(['a', 'b', 'c', 'd', 'e'], fill_value=0)

obj = Series(['blue', 'purple', 'yellow'], index=[0, 2, 4])
obj.reindex(range(6), method='ffill')

## Shortcut with ix()
frame
	Ohio Texas California
	a 0 1 2
	c 3 4 5
	d 6 7 8
frame.ix[['a', 'b', 'c', 'd'], states]
	a 1 NaN 2
	b NaN NaN NaN
	c 4 NaN 5
	d 7 NaN 8


Recipes
============================================================

Treating series like a dict
---------------------------
# Returns mean of cost for each state
data.groupby('state')['cost'].mean()
# Update dataframe with this series of mean cost
data['state'].map(data.groupby('state')['cost'].mean())